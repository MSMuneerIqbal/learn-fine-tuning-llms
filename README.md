# Fine-Tuning LLMs

This repo is part of the [Certified Cloud Native Applied Generative AI Engineer](https://docs.google.com/document/d/15usu1hkrrRLRjcq_3nCTT-0ljEcgiC44iSdvdqrCprk/edit?usp=sharing) program. It covers the fifth quarter of the course work:

## Quarter 5: Fine-Tuning Open-Source Large Language Models

This comprehensive course is designed to guide learners through the process of fine-tuning open-source Large Language Models (LLMs) such as Meta LLaMA 3 using PyTorch and Fast AI, with a particular emphasis on cloud-native training and deployment. The course covers everything from the fundamentals to advanced concepts, ensuring students acquire both theoretical knowledge and practical skills.
The journey begins with an introduction to LLMs, focusing on their architecture, capabilities, and the specific features of Meta LLaMA 3. Students will also set up their development environment, including tools like Anaconda, Jupyter Notebooks, PyTorch, and Fast AI, to prepare for hands-on learning.

Next, the course dives into PyTorch fundamentals, teaching students how to perform basic operations with tensors and build simple neural networks. This foundation is crucial for understanding the mechanics behind LLMs. The Fast AI library is introduced subsequently, highlighting its powerful data block API and tools for building and fine-tuning models. Practical sessions are integrated to ensure that students can apply these concepts effectively in real-world scenarios.

Data preparation is a crucial aspect of training models. The course covers comprehensive data collection and preprocessing techniques, such as tokenization and text normalisation. These steps are essential for preparing datasets suitable for fine-tuning LLMs like Meta LLaMA 3. Through practical exercises, students learn how to handle and preprocess various types of text data, ensuring they can prepare their datasets for optimal model performance.

Fine-tuning Meta LLaMA 3 with PyTorch forms a significant part of the course. Students will delve into the architecture of Meta LLaMA 3, learn how to load pre-trained models, and apply fine-tuning techniques. The course covers advanced topics such as regularisation and optimization strategies to enhance model performance. Practical sessions guide students through the entire fine-tuning process on custom datasets, emphasising best practices and troubleshooting techniques.

Building on these skills, the course then focuses on utilising Fast AI for NLP tasks. Students will learn how to leverage Fast AIâ€™s user-friendly interfaces and robust features to fine-tune Meta LLaMA 3 efficiently. Hands-on exercises ensure that students can effectively integrate Fast AI tools into their workflows, making the fine-tuning process more streamlined and accessible.

A critical aspect of this course is its focus on cloud-native training and deployment. Students will gain an understanding of cloud-native principles and infrastructure, setting up cloud environments on platforms like AWS, GCP, or Azure using Kubernetes. The course teaches how to train models using cloud resources, employing tools like Kubernetes and Kubeflow to optimise and distribute training workloads. Furthermore, students learn how to deploy models using Docker and Kubernetes, set up monitoring and maintenance tools, and ensure their models are scalable and efficient.

To round off the learning experience, the course includes an in-depth segment on exporting models for inference and building robust inference pipelines. Students will deploy models on cloud platforms, focusing on practical aspects of setting up monitoring tools to maintain model performance and reliability.

The course culminates in a capstone project, where students apply all the skills they have learned to fine-tune and deploy Meta LLaMA 3 on a chosen platform. This project allows students to demonstrate their understanding and proficiency in the entire process, from data preparation to cloud-native deployment. 
